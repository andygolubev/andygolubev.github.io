<!DOCTYPE html><html lang="en" class="__variable_5cfdac __variable_7620cf"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/569ce4b8f30dc480-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/6975cac0a7ae24f4-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" as="image" href="/images/sky/hero.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/logo.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic01.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic02.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic03.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic04.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic05.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic06.jpg"/><link rel="preload" as="image" href="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic07.jpg"/><link rel="stylesheet" href="/_next/static/css/8f74a3e5566f948c.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/7599afaaf9c9e399.css" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/598b09e17ec3b27b.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-06e7e4f151d607cb.js"/><script src="/_next/static/chunks/4bd1b696-d4d5eb693d0a7af9.js" async=""></script><script src="/_next/static/chunks/684-156f158a5850de9d.js" async=""></script><script src="/_next/static/chunks/main-app-5e733401fc257057.js" async=""></script><script src="/_next/static/chunks/874-1d5071fab5c23a5a.js" async=""></script><script src="/_next/static/chunks/63-b8ef6e972851fb15.js" async=""></script><script src="/_next/static/chunks/app/layout-128ffe95b5a5832b.js" async=""></script><script src="/_next/static/chunks/598-889c1586942b7842.js" async=""></script><script src="/_next/static/chunks/app/page-c42db5e253cc6e8c.js" async=""></script><meta name="next-size-adjust" content=""/><title>Andy Golubev | Cloud Architect &amp; DevOps Engineer</title><meta name="description" content="Personal website of AWS &amp; GCP certified Cloud Architect &amp; DevOps Engineer Andy Golubev. Certifications, GitHub work, and articles on Kubernetes and Terraform."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="48x48"/><link rel="icon" href="/icon.png?a0799f5d3aa48e9f" type="image/png" sizes="512x512"/><link rel="apple-touch-icon" href="/apple-icon.png?21c489252a03a1cc" type="image/png" sizes="180x180"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body><header class="Header_header__AF_3G"><div class="Header_rect__bQjpJ"></div><nav class="Header_nav__LVYU2"><a href="/">Home</a><a href="mailto:andygolubevcontact@gmail.com" target="_blank">Contact</a><a href="/articles/">Articles</a><a href="https://github.com/andygolubev" target="_blank">GitHub</a><a href="https://www.linkedin.com/in/andy-golubev/" target="_blank">LinkedIn</a></nav></header><div class="page_articleContainer__6YqF2"><h1 style="font-size:2rem">Backup tool using AWS Batch, ECS and Fargate for backuping objects from other clouds</h1>
<p><strong>Date: 13 February 2024</strong></p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/logo.jpg" alt="logo"/></figure>
<h2>Introduction</h2>
<p>Sometimes we have to do dull tasks like setting up a backup system. We want to do it in the easiest and cheapest way possible. That&#x27;s why I think AWS gives us lots of services to do it smoothly.</p>
<p>Because my computing stuff is on Digital Ocean, I couldn&#x27;t use AWS Backup. So I looked at services that can provide infrequent workload and don&#x27;t cost anything when they&#x27;re not busy. For this plan, I picked AWS Batch with ECS on Fargate. And I use Event Bridge scheduler to start the jobs.</p>
<h2>Solution</h2>
<p>This is a general picture of my solution.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic01.jpg" alt="solution diagram"/></figure>
<p>I&#x27;m setting up everything using AWS CloudFormation.</p>
<pre><code>...
    BackuperComputeEnvironment:
    Type: AWS::Batch::ComputeEnvironment
    Properties:
        Type: MANAGED
        ComputeEnvironmentName: backuper-environment
        ComputeResources:
        MaxvCpus: 4
        SecurityGroupIds:
            - !Ref BackuperSecurityGroup
        Type: FARGATE
        Subnets:
            - !Ref BackuperSubnet
        Tags: {&quot;Name&quot; : &quot;BackuperComputeEnvironment&quot;, &quot;CreatedBy&quot; : &quot;CloudFormationStack&quot;, &quot;App&quot; : &quot;Backuper&quot;}
        State: ENABLED

    BackuperJobDefinition:
    Type: AWS::Batch::JobDefinition
    Properties:
        Type: container
        JobDefinitionName: BackuperJobDefinition
        PlatformCapabilities:
        - FARGATE
        ContainerProperties:
        Image: registry.hub.docker.com/andygolubev/backuper:latest
        Environment:
            - Name: AWS_BACKUP_DESTINATION_BUCKET
            Value: !Ref awsBackupDestinationBucketName
            - Name: DO_PG_USER
            Value: !Ref doPgUser
            - Name: DO_KEY
            Value: !Ref doKey
            - Name: DO_PG_DBNAME
            Value: !Ref doPgDbname
            - Name: DO_PG_HOST
            Value: !Ref doPgHost
            - Name: DO_SECRET
            Value: !Ref doSecret
            - Name: DO_REGION_ENDPOINT
            Value: !Ref doRegionEndpoint
            - Name: DO_PG_PORT
            Value: !Ref doPgPort
            - Name: DO_PG_PASSWORD
            Value: !Ref doPgPassword
        Command:
            - /bin/bash
            - -c
            - /backuper/s3_backup_script.sh &amp;&amp; /bin/bash -c /backuper/postgre_backup_script.sh
        Privileged: false
        JobRoleArn: !GetAtt  BackuperAmazonECSTaskExecutionRole.Arn
        ExecutionRoleArn: !GetAtt BackuperAmazonECSTaskExecutionRole.Arn
        ReadonlyRootFilesystem: false
        NetworkConfiguration:
            AssignPublicIp: ENABLED
        ResourceRequirements:
            - Type: MEMORY
            Value: 1024
            - Type: VCPU
            Value: 0.5
        LogConfiguration:
            LogDriver: awslogs
            Options:
            &quot;awslogs-group&quot;: !Ref BackuperLogGroup
            &quot;awslogs-stream-prefix&quot;: &quot;prefix&quot;
        Tags: {&quot;Name&quot; : &quot;BackuperJobDefinition&quot;, &quot;CreatedBy&quot; : &quot;CloudFormationStack&quot;, &quot;App&quot; : &quot;Backuper&quot;}

...
</code></pre>
<p>I use my Docker Image with built-in bash scripts.</p>
<pre><code>FROM ubuntu:22.04
WORKDIR /tmp

# install tools
RUN apt update &amp;&amp; apt -y upgrade &amp;&amp; apt -y --no-install-suggests --no-install-recommends install wget unzip curl tree git jq gettext zip ca-certificates

# install aws cli v2
RUN curl &quot;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip&quot; -o &quot;awscliv2.zip&quot; &amp;&amp; \
    unzip awscliv2.zip &amp;&amp; \
    ./aws/install &amp;&amp; \ 
    aws --version

# install latest postgre tools
RUN apt -y install lsb-release gnupg2 --no-install-suggests --no-install-recommends &amp;&amp; \
    sh -c &#x27;echo &quot;deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main&quot; &gt; /etc/apt/sources.list.d/pgdg.list&#x27; &amp;&amp; \
    wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - &amp;&amp; \
    apt update &amp;&amp; \
    apt -y install postgresql-client

# make working folders
RUN mkdir /backuper; mkdir /backup; mkdir /backup_db
WORKDIR /backuper

# declare variables for s3_backup_script.sh
ENV DO_KEY=NOT_DEFINED
ENV DO_SECRET=NOT_DEFINED
ENV DO_REGION_ENDPOINT=NOT_DEFINED
ENV AWS_BACKUP_DESTINATION_BUCKET=NOT_DEFINED

# declare variables for postgre_backup_script.sh
ENV DO_PG_HOST=NOT_DEFINED
ENV DO_PG_PORT=NOT_DEFINED
ENV DO_PG_USER=NOT_DEFINED
ENV DO_PG_PASSWORD=NOT_DEFINED
ENV DO_PG_DBNAME=NOT_DEFINED

# create a buckets backup script inside the docker image
RUN echo &quot;BUCKETS_ALL=\$(AWS_ACCESS_KEY_ID=\$DO_KEY AWS_SECRET_ACCESS_KEY=\$DO_SECRET aws s3 ls --endpoint=\$DO_REGION_ENDPOINT  | awk &#x27;{print \$3}&#x27;)&quot; &gt;&gt; /backuper/s3_backup_script.sh
RUN echo &#x27;\
echo &quot;This buckets will be processed: $BUCKETS_ALL&quot; \n\
for BUCKET in $BUCKETS_ALL \n\
do \n\
    echo &quot;Processing bucket -&gt; $BUCKET&quot; \n\
    mkdir -p /backup/$BUCKET/ \n\
    AWS_ACCESS_KEY_ID=$DO_KEY AWS_SECRET_ACCESS_KEY=$DO_SECRET aws s3 cp --quiet --recursive --endpoint=$DO_REGION_ENDPOINT s3://$BUCKET /backup/$BUCKET/ \n\
    ZIP_FILE_DATE_TIME=$(date +%Y-%m-%d--%H-%M) \n\
    zip --recurse-paths --quiet /backup/$ZIP_FILE_DATE_TIME-UTC-$BUCKET-bucket_backup.zip /backup/$BUCKET/ \n\
    aws s3 cp --storage-class GLACIER_IR /backup/$ZIP_FILE_DATE_TIME-UTC-$BUCKET-bucket_backup.zip  s3://$AWS_BACKUP_DESTINATION_BUCKET \n\
    echo &quot;Successfully Processed -&gt; $BUCKET&quot; \n\
done \n\
echo &quot;Bucket backup is COMPLITED&quot; \
&#x27; &gt;&gt; /backuper/s3_backup_script.sh

# create a postgre backup script inside the docker image
RUN echo &#x27;\
echo &quot;Making dump for PostgreSQL Database --&gt; $DO_PG_DBNAME&quot; \n\
mkdir -p /backup_db/$DO_PG_DBNAME/ \n\
PGPASSWORD=$DO_PG_PASSWORD pg_dump -U $DO_PG_USER -h $DO_PG_HOST -p $DO_PG_PORT -Fc $DO_PG_DBNAME &gt; /backup_db/$DO_PG_DBNAME/$DO_PG_DBNAME.dump \n\
PGPASSWORD=$DO_PG_PASSWORD pg_dump -U $DO_PG_USER -h $DO_PG_HOST -p $DO_PG_PORT $DO_PG_DBNAME &gt; /backup_db/$DO_PG_DBNAME/$DO_PG_DBNAME.sql \n\
ZIP_FILE_DATE_TIME=$(date +%Y-%m-%d--%H-%M) \n\
zip --recurse-paths --quiet /backup_db/$ZIP_FILE_DATE_TIME-UTC-$DO_PG_DBNAME-postgre_backup.zip /backup_db/$DO_PG_DBNAME/ \n\
aws s3 cp --storage-class GLACIER_IR /backup_db/$ZIP_FILE_DATE_TIME-UTC-$DO_PG_DBNAME-postgre_backup.zip  s3://$AWS_BACKUP_DESTINATION_BUCKET \n\
echo &quot;Successfully Processed -&gt; $DO_PG_DBNAME&quot; \n\
echo &quot;Postgre backup is COMPLITED&quot; \
&#x27; &gt;&gt; /backuper/postgre_backup_script.sh

# make the script runnable
RUN chmod +x /backuper/s3_backup_script.sh
RUN chmod +x /backuper/postgre_backup_script.sh

ENTRYPOINT [&quot;/bin/bash&quot;, &quot;-c&quot;,  &quot;/backuper/s3_backup_script.sh &amp;&amp; /bin/bash -c /backuper/postgre_backup_script.sh&quot;]
</code></pre>
<p>Once the CloudFormation stack is deployed successfully, we can see all the resources that have been created.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic02.jpg" alt="resources"/></figure>
<p>And as you can see, our AWS Batch service is fully set up and waiting for a trigger event.</p>
<p>The scheduler is set with these configurations, and the cron will initiate an event at 8 AM UTC.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic03.jpg" alt="scheduler"/></figure>
<p>We have an Event Bridge rule that sifts through Batch Job-related events, keeping sensitive data out, and then sends them to the SNS Topic.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic04.jpg" alt="event bridge rule"/></figure>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic05.jpg" alt="event bridge rule details"/></figure>
<p>After running our job (either manually or by the scheduler), we can see it on our dashboard and receive emails with the job&#x27;s status.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic06.jpg" alt="job dashboard"/></figure>
<p>The result notification looks like this.</p>
<figure style="margin:2rem 0"><img src="/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic07.jpg" alt="notification email"/></figure>
<h2>Conclusion</h2>
<p>Here&#x27;s an example of how you can handle occasional workloads efficiently with AWS Batch + ECS + Fargate, all while keeping costs down. Give it a shot!</p>
<p>I hope you enjoy this article.</p>
<p>You can find all of my code in my GitHub repository: <a href="https://github.com/andygolubev/aws-backup-with-batch-and-fargate">https://github.com/andygolubev/aws-backup-with-batch-and-fargate</a></p>
<p>Feel free to connect with me on LinkedIn: <a href="https://www.linkedin.com/in/andy-golubev/">https://www.linkedin.com/in/andy-golubev/</a></p></div><!--$--><!--/$--><!--$--><!--/$--><footer class="Footer_footer__4vzqH"><div class="Footer_footerContainer__77_mg"><div class="DisintegratingImage_container__wGJdA Footer_footerBackgroundImage__aXnu1"><img src="/images/sky/hero.jpg" alt="Footer background" class="DisintegratingImage_image__qNdyj" style="opacity:1;transition:opacity 0.5s ease-out"/></div><h2>Run in the cloud. Reach the world.</h2><div class="Footer_footerLinks__xuRtG"><a href="https://github.com/andygolubev" target="_blank"><img alt="GitHub" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" style="color:transparent" src="/images/icons/github.svg"/></a><a href="https://www.linkedin.com/in/andy-golubev/" target="_blank"><img alt="LinkedIn" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" style="color:transparent" src="/images/icons/linkedin.svg"/></a><a href="mailto:andygolubevcontact@gmail.com" target="_blank"><img alt="Send email" loading="lazy" width="24" height="24" decoding="async" data-nimg="1" style="color:transparent" src="/images/icons/email.svg"/></a></div></div></footer><div class="Credits_credits__Spqox"><a href="https://www.linkedin.com/in/olgagolubev/" target="_blank">Developed and designed by Olga Golubev</a></div><script src="/_next/static/chunks/webpack-06e7e4f151d607cb.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[2784,[\"874\",\"static/chunks/874-1d5071fab5c23a5a.js\",\"63\",\"static/chunks/63-b8ef6e972851fb15.js\",\"177\",\"static/chunks/app/layout-128ffe95b5a5832b.js\"],\"default\"]\n3:I[7555,[],\"\"]\n4:I[1295,[],\"\"]\n5:I[4557,[\"874\",\"static/chunks/874-1d5071fab5c23a5a.js\",\"63\",\"static/chunks/63-b8ef6e972851fb15.js\",\"598\",\"static/chunks/598-889c1586942b7842.js\",\"974\",\"static/chunks/app/page-c42db5e253cc6e8c.js\"],\"default\"]\n6:I[3063,[\"874\",\"static/chunks/874-1d5071fab5c23a5a.js\",\"63\",\"static/chunks/63-b8ef6e972851fb15.js\",\"598\",\"static/chunks/598-889c1586942b7842.js\",\"974\",\"static/chunks/app/page-c42db5e253cc6e8c.js\"],\"Image\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/media/569ce4b8f30dc480-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/6975cac0a7ae24f4-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/8f74a3e5566f948c.css\",\"style\"]\n:HL[\"/_next/static/css/7599afaaf9c9e399.css\",\"style\"]\n:HL[\"/_next/static/css/598b09e17ec3b27b.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"O8H6bF7pcj2mznJgXipHh\",\"p\":\"\",\"c\":[\"\",\"articles\",\"backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"articles\",{\"children\":[[\"slug\",\"backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8f74a3e5566f948c.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"link\",\"1\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/7599afaaf9c9e399.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"__variable_5cfdac __variable_7620cf\",\"children\":[\"$\",\"body\",null,{\"children\":[[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}],[\"$\",\"footer\",null,{\"className\":\"Footer_footer__4vzqH\",\"children\":[\"$\",\"div\",null,{\"className\":\"Footer_footerContainer__77_mg\",\"children\":[[\"$\",\"$L5\",null,{\"src\":\"/images/sky/hero.jpg\",\"alt\":\"Footer background\",\"className\":\"Footer_footerBackgroundImage__aXnu1\"}],[\"$\",\"h2\",null,{\"children\":\"Run in the cloud. Reach the world.\"}],[\"$\",\"div\",null,{\"className\":\"Footer_footerLinks__xuRtG\",\"children\":[[\"$\",\"a\",null,{\"href\":\"https://github.com/andygolubev\",\"target\":\"_blank\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/icons/github.svg\",\"alt\":\"GitHub\",\"width\":24,\"height\":24}]}],[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/andy-golubev/\",\"target\":\"_blank\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/icons/linkedin.svg\",\"alt\":\"LinkedIn\",\"width\":24,\"height\":24}]}],[\"$\",\"a\",null,{\"href\":\"mailto:andygolubevcontact@gmail.com\",\"target\":\"_blank\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/icons/email.svg\",\"alt\":\"Send email\",\"width\":24,\"height\":24}]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"Credits_credits__Spqox\",\"children\":[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/olgagolubev/\",\"target\":\"_blank\",\"children\":\"Developed and designed by Olga Golubev\"}]}]]}]}]]}],{\"children\":[\"articles\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/598b09e17ec3b27b.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"2HTzPWlyb2PGGUVebuyh6\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"15:T8ea,"])</script><script>self.__next_f.push([1,"...\n    BackuperComputeEnvironment:\n    Type: AWS::Batch::ComputeEnvironment\n    Properties:\n        Type: MANAGED\n        ComputeEnvironmentName: backuper-environment\n        ComputeResources:\n        MaxvCpus: 4\n        SecurityGroupIds:\n            - !Ref BackuperSecurityGroup\n        Type: FARGATE\n        Subnets:\n            - !Ref BackuperSubnet\n        Tags: {\"Name\" : \"BackuperComputeEnvironment\", \"CreatedBy\" : \"CloudFormationStack\", \"App\" : \"Backuper\"}\n        State: ENABLED\n\n    BackuperJobDefinition:\n    Type: AWS::Batch::JobDefinition\n    Properties:\n        Type: container\n        JobDefinitionName: BackuperJobDefinition\n        PlatformCapabilities:\n        - FARGATE\n        ContainerProperties:\n        Image: registry.hub.docker.com/andygolubev/backuper:latest\n        Environment:\n            - Name: AWS_BACKUP_DESTINATION_BUCKET\n            Value: !Ref awsBackupDestinationBucketName\n            - Name: DO_PG_USER\n            Value: !Ref doPgUser\n            - Name: DO_KEY\n            Value: !Ref doKey\n            - Name: DO_PG_DBNAME\n            Value: !Ref doPgDbname\n            - Name: DO_PG_HOST\n            Value: !Ref doPgHost\n            - Name: DO_SECRET\n            Value: !Ref doSecret\n            - Name: DO_REGION_ENDPOINT\n            Value: !Ref doRegionEndpoint\n            - Name: DO_PG_PORT\n            Value: !Ref doPgPort\n            - Name: DO_PG_PASSWORD\n            Value: !Ref doPgPassword\n        Command:\n            - /bin/bash\n            - -c\n            - /backuper/s3_backup_script.sh \u0026\u0026 /bin/bash -c /backuper/postgre_backup_script.sh\n        Privileged: false\n        JobRoleArn: !GetAtt  BackuperAmazonECSTaskExecutionRole.Arn\n        ExecutionRoleArn: !GetAtt BackuperAmazonECSTaskExecutionRole.Arn\n        ReadonlyRootFilesystem: false\n        NetworkConfiguration:\n            AssignPublicIp: ENABLED\n        ResourceRequirements:\n            - Type: MEMORY\n            Value: 1024\n            - Type: VCPU\n            Value: 0.5\n        LogConfiguration:\n            LogDriver: awslogs\n            Options:\n            \"awslogs-group\": !Ref BackuperLogGroup\n            \"awslogs-stream-prefix\": \"prefix\"\n        Tags: {\"Name\" : \"BackuperJobDefinition\", \"CreatedBy\" : \"CloudFormationStack\", \"App\" : \"Backuper\"}\n\n...\n"])</script><script>self.__next_f.push([1,"16:Td57,"])</script><script>self.__next_f.push([1,"FROM ubuntu:22.04\nWORKDIR /tmp\n\n# install tools\nRUN apt update \u0026\u0026 apt -y upgrade \u0026\u0026 apt -y --no-install-suggests --no-install-recommends install wget unzip curl tree git jq gettext zip ca-certificates\n\n# install aws cli v2\nRUN curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" \u0026\u0026 \\\n    unzip awscliv2.zip \u0026\u0026 \\\n    ./aws/install \u0026\u0026 \\ \n    aws --version\n\n# install latest postgre tools\nRUN apt -y install lsb-release gnupg2 --no-install-suggests --no-install-recommends \u0026\u0026 \\\n    sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" \u003e /etc/apt/sources.list.d/pgdg.list' \u0026\u0026 \\\n    wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | apt-key add - \u0026\u0026 \\\n    apt update \u0026\u0026 \\\n    apt -y install postgresql-client\n\n# make working folders\nRUN mkdir /backuper; mkdir /backup; mkdir /backup_db\nWORKDIR /backuper\n\n# declare variables for s3_backup_script.sh\nENV DO_KEY=NOT_DEFINED\nENV DO_SECRET=NOT_DEFINED\nENV DO_REGION_ENDPOINT=NOT_DEFINED\nENV AWS_BACKUP_DESTINATION_BUCKET=NOT_DEFINED\n\n# declare variables for postgre_backup_script.sh\nENV DO_PG_HOST=NOT_DEFINED\nENV DO_PG_PORT=NOT_DEFINED\nENV DO_PG_USER=NOT_DEFINED\nENV DO_PG_PASSWORD=NOT_DEFINED\nENV DO_PG_DBNAME=NOT_DEFINED\n\n# create a buckets backup script inside the docker image\nRUN echo \"BUCKETS_ALL=\\$(AWS_ACCESS_KEY_ID=\\$DO_KEY AWS_SECRET_ACCESS_KEY=\\$DO_SECRET aws s3 ls --endpoint=\\$DO_REGION_ENDPOINT  | awk '{print \\$3}')\" \u003e\u003e /backuper/s3_backup_script.sh\nRUN echo '\\\necho \"This buckets will be processed: $BUCKETS_ALL\" \\n\\\nfor BUCKET in $BUCKETS_ALL \\n\\\ndo \\n\\\n    echo \"Processing bucket -\u003e $BUCKET\" \\n\\\n    mkdir -p /backup/$BUCKET/ \\n\\\n    AWS_ACCESS_KEY_ID=$DO_KEY AWS_SECRET_ACCESS_KEY=$DO_SECRET aws s3 cp --quiet --recursive --endpoint=$DO_REGION_ENDPOINT s3://$BUCKET /backup/$BUCKET/ \\n\\\n    ZIP_FILE_DATE_TIME=$(date +%Y-%m-%d--%H-%M) \\n\\\n    zip --recurse-paths --quiet /backup/$ZIP_FILE_DATE_TIME-UTC-$BUCKET-bucket_backup.zip /backup/$BUCKET/ \\n\\\n    aws s3 cp --storage-class GLACIER_IR /backup/$ZIP_FILE_DATE_TIME-UTC-$BUCKET-bucket_backup.zip  s3://$AWS_BACKUP_DESTINATION_BUCKET \\n\\\n    echo \"Successfully Processed -\u003e $BUCKET\" \\n\\\ndone \\n\\\necho \"Bucket backup is COMPLITED\" \\\n' \u003e\u003e /backuper/s3_backup_script.sh\n\n# create a postgre backup script inside the docker image\nRUN echo '\\\necho \"Making dump for PostgreSQL Database --\u003e $DO_PG_DBNAME\" \\n\\\nmkdir -p /backup_db/$DO_PG_DBNAME/ \\n\\\nPGPASSWORD=$DO_PG_PASSWORD pg_dump -U $DO_PG_USER -h $DO_PG_HOST -p $DO_PG_PORT -Fc $DO_PG_DBNAME \u003e /backup_db/$DO_PG_DBNAME/$DO_PG_DBNAME.dump \\n\\\nPGPASSWORD=$DO_PG_PASSWORD pg_dump -U $DO_PG_USER -h $DO_PG_HOST -p $DO_PG_PORT $DO_PG_DBNAME \u003e /backup_db/$DO_PG_DBNAME/$DO_PG_DBNAME.sql \\n\\\nZIP_FILE_DATE_TIME=$(date +%Y-%m-%d--%H-%M) \\n\\\nzip --recurse-paths --quiet /backup_db/$ZIP_FILE_DATE_TIME-UTC-$DO_PG_DBNAME-postgre_backup.zip /backup_db/$DO_PG_DBNAME/ \\n\\\naws s3 cp --storage-class GLACIER_IR /backup_db/$ZIP_FILE_DATE_TIME-UTC-$DO_PG_DBNAME-postgre_backup.zip  s3://$AWS_BACKUP_DESTINATION_BUCKET \\n\\\necho \"Successfully Processed -\u003e $DO_PG_DBNAME\" \\n\\\necho \"Postgre backup is COMPLITED\" \\\n' \u003e\u003e /backuper/postgre_backup_script.sh\n\n# make the script runnable\nRUN chmod +x /backuper/s3_backup_script.sh\nRUN chmod +x /backuper/postgre_backup_script.sh\n\nENTRYPOINT [\"/bin/bash\", \"-c\",  \"/backuper/s3_backup_script.sh \u0026\u0026 /bin/bash -c /backuper/postgre_backup_script.sh\"]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"page_articleContainer__6YqF2\",\"children\":[[\"$\",\"h1\",null,{\"style\":{\"fontSize\":\"2rem\"},\"children\":\"Backup tool using AWS Batch, ECS and Fargate for backuping objects from other clouds\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"$\",\"strong\",null,{\"children\":\"Date: 13 February 2024\"}]}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/logo.jpg\",\"alt\":\"logo\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Introduction\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Sometimes we have to do dull tasks like setting up a backup system. We want to do it in the easiest and cheapest way possible. That's why I think AWS gives us lots of services to do it smoothly.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Because my computing stuff is on Digital Ocean, I couldn't use AWS Backup. So I looked at services that can provide infrequent workload and don't cost anything when they're not busy. For this plan, I picked AWS Batch with ECS on Fargate. And I use Event Bridge scheduler to start the jobs.\"}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Solution\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"This is a general picture of my solution.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic01.jpg\",\"alt\":\"solution diagram\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I'm setting up everything using AWS CloudFormation.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"$15\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I use my Docker Image with built-in bash scripts.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"$16\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Once the CloudFormation stack is deployed successfully, we can see all the resources that have been created.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic02.jpg\",\"alt\":\"resources\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"And as you can see, our AWS Batch service is fully set up and waiting for a trigger event.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The scheduler is set with these configurations, and the cron will initiate an event at 8 AM UTC.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic03.jpg\",\"alt\":\"scheduler\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We have an Event Bridge rule that sifts through Batch Job-related events, keeping sensitive data out, and then sends them to the SNS Topic.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic04.jpg\",\"alt\":\"event bridge rule\"}]}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic05.jpg\",\"alt\":\"event bridge rule details\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"After running our job (either manually or by the scheduler), we can see it on our dashboard and receive emails with the job's status.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic06.jpg\",\"alt\":\"job dashboard\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"The result notification looks like this.\"}],\"\\n\",[\"$\",\"figure\",null,{\"style\":{\"margin\":\"2rem 0\"},\"children\":[\"$\",\"img\",null,{\"src\":\"/articles/backup-tool-using-aws-batch-ecs-and-fargate-for-backuping-objects-from-other-clouds/pic07.jpg\",\"alt\":\"notification email\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Conclusion\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Here's an example of how you can handle occasional workloads efficiently with AWS Batch + ECS + Fargate, all while keeping costs down. Give it a shot!\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"I hope you enjoy this article.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"You can find all of my code in my GitHub repository: \",[\"$\",\"a\",null,{\"href\":\"https://github.com/andygolubev/aws-backup-with-batch-and-fargate\",\"children\":\"https://github.com/andygolubev/aws-backup-with-batch-and-fargate\"}]]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Feel free to connect with me on LinkedIn: \",[\"$\",\"a\",null,{\"href\":\"https://www.linkedin.com/in/andy-golubev/\",\"children\":\"https://www.linkedin.com/in/andy-golubev/\"}]]}]]}]\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Andy Golubev | Cloud Architect \u0026 DevOps Engineer\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Personal website of AWS \u0026 GCP certified Cloud Architect \u0026 DevOps Engineer Andy Golubev. Certifications, GitHub work, and articles on Kubernetes and Terraform.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"48x48\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/icon.png?a0799f5d3aa48e9f\",\"type\":\"image/png\",\"sizes\":\"512x512\"}],[\"$\",\"link\",\"4\",{\"rel\":\"apple-touch-icon\",\"href\":\"/apple-icon.png?21c489252a03a1cc\",\"type\":\"image/png\",\"sizes\":\"180x180\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>